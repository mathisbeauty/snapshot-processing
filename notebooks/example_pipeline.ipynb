{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SingularityDAO snapshot processing pipeline\r\n",
    "\r\n",
    "This notebook it's meant to provide an explanation over the thought process behind the pipeline design and \r\n",
    "\r\n",
    "The code only version can be found at `./scripts/pipeline.py`.\r\n",
    "\r\n",
    "## Scalability\r\n",
    "\r\n",
    "The main purpose of this notebook it's to explore how to automate the data processing of the snapshots.\r\n",
    "\r\n",
    "That's the reason why we are only dealing with a subset of the holders.\r\n",
    "\r\n",
    "Once the process works reliably and can be performed automatically, the next step would be to containerize it for cloud processing.\r\n",
    "\r\n",
    "In principle, the processing could be distributed by grouping all the addresses to process in batches of N addresses, and each container would calculate the eligibility for one batch at a time. Once the batch it's processed, the container would either request another one, or be destroyed if there aren't more.\r\n",
    "\r\n",
    "## 1. Take snapshots\r\n",
    "\r\n",
    "For the example pipeline, I'm going to use just a couple of snapshots taken manually, due to the fact that the main focus of this notebook is to create the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e11c16bd8665eb0b93b5a4e36de9de775b152bd48248748b339502fe1d55318"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}