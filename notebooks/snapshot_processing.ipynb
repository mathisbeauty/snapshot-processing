{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SingularityDAO: Automating the airdrop calculations\r\n",
    "\r\n",
    "## Overview\r\n",
    "\r\n",
    "1. **Take Snapshots** Generate snapshots placed at random intervals (and optionally store results in a DB)\r\n",
    "2. **Calculate Eligibility**  Categorize each address and apply custom logic depending on the airdrop's eligibility criteria \r\n",
    "3. **Serve To The Portal** Store eligibility of each type of airdrop per user in a DB and make it accessible to the portal\r\n",
    "\r\n",
    "## 1. Take Snapshots\r\n",
    "\r\n",
    "### Using a database and an ethereum node\r\n",
    "\r\n",
    "A relational DB can be used to store a list of balances. I think it's what could be currently implemented in the existing airdrop.\r\n",
    "\r\n",
    "The balances in the database could be bootstrapped by an initial import from Etherscan or by the initial snapshot, and be kept up to date by running a node that listens for new transactions.\r\n",
    "\r\n",
    "## 2. Calculate Eligibility\r\n",
    "\r\n",
    "The following criteria follows the airdrop mechanics as described in the [official post](https://medium.com/singularitydao/singularitydao-updated-airdrop-eb6fc9d2210b)\r\n",
    "\r\n",
    "The airdrop mechanics encompass 3 different groups:\r\n",
    "- Stakers\r\n",
    "- Holders\r\n",
    "- Liquidity Providers\r\n",
    "\r\n",
    "The eligibility and distribution of each category are calculated separately.\r\n",
    "\r\n",
    "### Stakers\r\n",
    "\r\n",
    "#### Eligibility\r\n",
    "\r\n",
    "For stakers, the snapshots correspond directly to the staking windows, which happen monthly.\r\n",
    "\r\n",
    "To calculate the eligibility of a staker, the main criteria is whether they were part of the snapshot on the 17th of April 2021, at 23:59 UTC+0.\r\n",
    "\r\n",
    "A staker it's eligible if they're staking on all the snapshots taken.\r\n",
    "\r\n",
    "#### Distribution\r\n",
    "\r\n",
    "2,000,000 SDAO tokens are distributed among the stakers:\r\n",
    "- 1,000,000 distributed proportionally to the eligible wallets that claim the airdrop, without taking into account stake, only number of participants\r\n",
    "- 1,000,000 distributed proportionally to the amount staked\r\n",
    "\r\n",
    "### Holders and Liquidity Providers\r\n",
    "\r\n",
    "Holders and Liquidity Providers are rewarded by the following formula:\r\n",
    "`Reward = total_reward * log10(1+user_balance) / SUM(log10(1+user_balance))`\r\n",
    "\r\n",
    "#### Eligility\r\n",
    "\r\n",
    "There are two sets of random snapshots taken, one for Holders and another one for Liquidity Providers.\r\n",
    "\r\n",
    "My only question here it's whether the liquidity providers eligibility it's calculated by their actual AGIX in Uniswap, or by converting the ETH & AGIX balances to fiat and calculating the AGIX that it's worth at the market price of when the snapshot was taken.\r\n",
    "\r\n",
    "#### Distribution\r\n",
    "\r\n",
    "In the two remaining months, July and August, 1,800,000 SDAO are left to be distributed. 825,000 in July and 975,000 in August.\r\n",
    "\r\n",
    "Once Liquidity Providers and Holders pass their respective eligibility criterio, they are added to the same pool of tokens.\r\n",
    "\r\n",
    "The reward for each user it's calculated by the ratio of the user's balance to the total balance of all the eligible users, smoothing both values with the `log10` function.\r\n",
    "Finally, the final reward it's the result of multiplying the previous value by the total reward for that month.\r\n",
    "\r\n",
    "## 3. Serve To The Portal\r\n",
    "\r\n",
    "Once the eligibility criteria of all users has been processed, the results can be imported into a DB to be served to the portal using GraphQL on the client side.\r\n",
    "\r\n",
    "# Final Notes\r\n",
    "\r\n",
    "## Distributing the Airdrop\r\n",
    "\r\n",
    "To the best of my knowledge, there are two ways in which claiming the airdrod could be accomplished in the current system.\r\n",
    "In both ways, I think that the server has to have a private key to sign a message, which approves X amount for the eligible wallet.\r\n",
    "\r\n",
    "- **On-chain:** The eligible wallets are registered on-chain using the server/admin's private key. The gas cost is payed by the server and the user.\r\n",
    "- **Off-chain:** the server signs a message on the fly when the user claims the airdrop. The user then appends the message to the claiming transaction. The gas cost is payed only by the user.\r\n",
    "\r\n",
    "## Manually getting the data\r\n",
    "\r\n",
    "Due to the fact that AGIX has less than 100,000 holders, we can manually download a .csv file with the balances of all holders [using Etherscan](https://etherscan.io/exportData?type=tokenholders&contract=0x5b7533812759b45c2b44c19e320ba2cd2681b542&decimal=8).\r\n",
    "\r\n",
    "This is doable due to the small number of holders, but not really practical for automating the production pipeline. As this is just an example, it will work for our purposes here.\r\n",
    "\r\n",
    "## Scalability\r\n",
    "\r\n",
    "The main purpose of this notebook it's to explore how to automate the data processing of the snapshots.\r\n",
    "\r\n",
    "That's the reason why we are only dealing with a subset of the snapshots.\r\n",
    "\r\n",
    "Once the process works reliably and can be performed automatically, the next step would be to containerize it for cloud processing.\r\n",
    "\r\n",
    "In principle, the processing could be distributed by grouping all the addresses to process in batches of N addresses, and each container would calculate the eligibility for one batch at a time. Once the batch it's processed, the container would either request another one, or be destroyed if there aren't more.\r\n",
    "\r\n",
    "This shouldn't be necessary if the amount of the data it's relatively small and the calculations can be done in a single machine quickly.\r\n",
    "\r\n",
    "## E2E Testing\r\n",
    "\r\n",
    "Once the processing of the airdrop has been automated with its own pipeline and the final eligibility of all addresses is in the DB, what remains is to test that all the other systems work as expected.\r\n",
    "\r\n",
    "For that, the E2E testing tool would be used (currently WIP).\r\n",
    "\r\n",
    "[You can find out more about the E2E testing tool here](https://github.com/mathisbeauty/e2e-tester)\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e11c16bd8665eb0b93b5a4e36de9de775b152bd48248748b339502fe1d55318"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}