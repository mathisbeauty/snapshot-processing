{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SingularityDAO snapshot processing pipeline\n",
    "\n",
    "This notebook it's meant to explore how the snapshots could be processed automatically.\n",
    "\n",
    "[//]: <> (The code-only version can be found at `./scripts/pipeline.py`.)\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Manually getting the data\n",
    "\n",
    "Due to the fact that AGIX has less than 100,000 holders, we can manually download a .csv file with the balances of all holders [using Etherscan](https://etherscan.io/exportData?type=tokenholders&contract=0x5b7533812759b45c2b44c19e320ba2cd2681b542&decimal=8).\n",
    "\n",
    "This is doable due to the small number of holders, but not really practical for automating the production pipeline. As this is just an example, it will work for our purposes here.\n",
    "\n",
    "### Scalability\n",
    "\n",
    "The main purpose of this notebook it's to explore how to automate the data processing of the snapshots.\n",
    "\n",
    "That's the reason why we are only dealing with a subset of the snapshots.\n",
    "\n",
    "Once the process works reliably and can be performed automatically, the next step would be to containerize it for cloud processing.\n",
    "\n",
    "In principle, the processing could be distributed by grouping all the addresses to process in batches of N addresses, and each container would calculate the eligibility for one batch at a time. Once the batch it's processed, the container would either request another one, or be destroyed if there aren't more.\n",
    "\n",
    "This shouldn't be necessary if the amount of the data it's relatively small and the calculations can be done in a single machine quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take snapshots\n",
    "\n",
    "For the example pipeline, I'm going to use just a small number of snapshots taken manually, due to the fact that the main focus of this notebook is to create the processing pipeline, not to gather the snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import all the libraries that we're going to use for the data processing and for gathering insights about the dataset with statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Pandas for tabular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import walk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is reading the `data` directory to see how many snapshots we have for each token respectively (AGI and AGIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12700788-export-tokenholders-for-contract-0x8eb24319393716668d768dcec29356ae9cffe285.csv']\n",
      "['12700800-export-tokenholders-for-contract-0x5b7533812759b45c2b44c19e320ba2cd2681b542.csv',\n",
      " '12709185-export-tokenholders-for-contract-0x5b7533812759b45c2b44c19e320ba2cd2681b542.csv',\n",
      " '12709949-export-tokenholders-for-contract-0x5b7533812759b45c2b44c19e320ba2cd2681b542.csv']\n"
     ]
    }
   ],
   "source": [
    "def get_snapshots(path):\n",
    "    return next(walk(path), (None, None, []))[2]\n",
    "\n",
    "agi_snapshots_files = get_snapshots('../data/holders/agi')\n",
    "agix_snapshots_files = get_snapshots('../data/holders/agix')\n",
    "\n",
    "pprint(agi_snapshots_files)\n",
    "pprint(agix_snapshots_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load snapshots for liquidity providers and stakers.\n",
    "\n",
    "In this example, the snapshots are the same as the AGIX holders, to focus on developing the calculations first, leaving getting the data from the database for later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_snapshots_files = get_snapshots('../data/lp')\n",
    "\n",
    "stakers_snapshots_files = get_snapshots('../data/stakers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the snapshots' contents using `pandas` and convert balances to unsinged long numbers (SDAO wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decimals AGI/AGIX\n",
    "\n",
    "DECIMALS_AGI = 8\n",
    "\n",
    "CONVERT_TO_FULL_BALANCE_AGI = np.power(np.ulonglong(10), DECIMALS_AGI)\n",
    "\n",
    "AGI_THRESHOLD = 1000 * CONVERT_TO_FULL_BALANCE_AGI\n",
    "\n",
    "def read_csv(folder, file):\n",
    "    # Read csv file\n",
    "    data_frame = pd.read_csv('../data/%s/%s' % (folder, file))\n",
    "    # Sort accounts by holding amount, larger holders at the top\n",
    "    data_frame = data_frame.astype({'Balance': np.ulonglong})\n",
    "    data_frame['Balance'] = data_frame['Balance'].apply(lambda x: x * CONVERT_TO_FULL_BALANCE_AGI)\n",
    "    data_frame = data_frame.sort_values('Balance', ascending=False)\n",
    "    return data_frame\n",
    "\n",
    "agi_snapshots_raw = [read_csv(\"holders\", \"agi/\" + file) for file in agi_snapshots_files]\n",
    "agix_snapshots_raw = [read_csv(\"holders\", \"agix/\" + file) for file in agix_snapshots_files]\n",
    "\n",
    "lp_snapshots_raw = [read_csv(\"lp\", file) for file in lp_snapshots_files]\n",
    "stakers_snapshots_raw = [read_csv(\"stakers\", file) for file in stakers_snapshots_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snapshots are now loaded as a panda DataFrame.\n",
    "\n",
    "Let's see the structure of a single snapshot and a single row, to get a better idea of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HolderAddress', 'Balance', 'PendingBalanceUpdate'], dtype='object')\n",
      "HolderAddress           0xbe0eb53f46cd790cd13851d5eff43d12404d33e8\n",
      "Balance                                        14182336000000000.0\n",
      "PendingBalanceUpdate                                            No\n",
      "Name: 8470, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(agi_snapshots_raw[0].columns)\n",
    "print(agi_snapshots_raw[0].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the `PendingBalanceUpdate` column and rename the other two, to clean the dataset and make it more practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['address', 'balance'], dtype='object')\n",
      "address    0xbe0eb53f46cd790cd13851d5eff43d12404d33e8\n",
      "balance                           14182336000000000.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clear_snapshot(snapshot):\n",
    "    cleaned_snapshot = snapshot.drop('PendingBalanceUpdate', axis=\"columns\")\n",
    "    cleaned_snapshot = cleaned_snapshot.rename(columns={\"HolderAddress\": \"address\", \"Balance\": \"balance\"})\n",
    "    cleaned_snapshot = cleaned_snapshot.reset_index(drop=True)\n",
    "    return cleaned_snapshot\n",
    "\n",
    "agi_snapshots = [clear_snapshot(snapshot) for snapshot in agi_snapshots_raw]\n",
    "agix_snapshots = [clear_snapshot(snapshot) for snapshot in agix_snapshots_raw]\n",
    "\n",
    "lp_snapshots = [clear_snapshot(snapshot) for snapshot in lp_snapshots_raw]\n",
    "stakers_snapshots = [clear_snapshot(snapshot) for snapshot in stakers_snapshots_raw]\n",
    "\n",
    "print(agi_snapshots[0].columns)\n",
    "# Address and balance from the account with the largest holding, one of the Binance wallets\n",
    "print(agi_snapshots[0].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate eligibility\n",
    "\n",
    "With the snapshot data ready, we can start to calculate the eligible addresses.\n",
    "\n",
    "Note that the following is intended to illustrate the process, but with the actual dataset of snapshots, a similar but more complex processing would be applied.\n",
    "\n",
    "Additionally, for the sake of the example, this process will only take into account holders, ignoring stakers and liquidity providers.\n",
    "\n",
    "Nonetheless, it's feasible to adapt this same series of steps to stakers and liquidity providers, by preparing their respective datasets as described in step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial snapshot\n",
    "\n",
    "There's an initial snapshot that delimits how many addresses are eligible for the airdrop.\n",
    "\n",
    "In my case it's the snapshot of the frozen AGI balances, but in the airdrop it would be the snapshot from 17th of April 2021, at 23:59 UTC+0.\n",
    "\n",
    "Let's create a subset based on the addresses from the first snapshot that have more than 1.000 AGI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGI Snapshots: 1\n",
      "AGIX Snapshots: 3\n",
      "LP Snapshots: 2\n",
      "Stakers Snapshots: 2\n",
      "\n",
      "Total Addresses (holders): 25979\n",
      "Eligible Addresses (holders): 16689\n",
      "\n",
      "Total Addresses (LP): 24414\n",
      "Eligible Addresses (LP): 16470\n",
      "\n",
      "Total Addresses (stakers): 24414\n",
      "Eligible Addresses (stakers): 16470\n",
      "\n",
      "\n",
      "address    0xba3699d5fc6276e87c3a761eaaf82162d4a86854\n",
      "balance                                100000000000.0\n",
      "Name: 16688, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"AGI Snapshots: %s\" % len(agi_snapshots))\n",
    "print(\"AGIX Snapshots: %s\" % len(agix_snapshots))\n",
    "print(\"LP Snapshots: %s\" % len(lp_snapshots))\n",
    "print(\"Stakers Snapshots: %s\" % len(stakers_snapshots))\n",
    "print()\n",
    "\n",
    "# Get the first snapshot and use it as the starting point for the calculations\n",
    "def get_initial(initial_snapshot, category):\n",
    "    total_addresses = len(initial_snapshot.index)\n",
    "    eligible_addresses_initial = initial_snapshot[initial_snapshot['balance'] >= AGI_THRESHOLD]\n",
    "    \n",
    "    print('Total Addresses (%s): %s' % (category, total_addresses))\n",
    "    print('Eligible Addresses (%s): %s' % (category, len(eligible_addresses_initial.index)))\n",
    "    print()\n",
    "    \n",
    "    return eligible_addresses_initial\n",
    "\n",
    "eligible_addresses_holders = get_initial(agi_snapshots[0], 'holders')\n",
    "eligible_addresses_lp = get_initial(lp_snapshots[0], 'LP')\n",
    "eligible_addresses_stakers = get_initial(stakers_snapshots[0], 'stakers')\n",
    "\n",
    "print()\n",
    "# Print address with smaller eligible balance\n",
    "print(eligible_addresses_holders.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from the initial ~26k addresses, only 16689 pass the threshold to be eligible.\n",
    "\n",
    "Now, it's a matter of iterating through the remaining snapshots using this initial set of accounts, and checking if the accounts are still eligible, removing the ones that are below the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the snapshots\n",
    "\n",
    "First, let's merge all snapshots (AGI and AGIX) into a single array and discard the first one, as that one it's already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total snapshots (including first one): 4\n",
      "Total snapshots (without including first one): 3\n"
     ]
    }
   ],
   "source": [
    "# Merge snapshots\n",
    "holders_snapshots = agi_snapshots + agix_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we iterate over the snapshots, filtering the initial set of eligible accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Eligible Addresses (holders): 16689\n",
      "\n",
      "Snapshot #0\n",
      "Eligible Addresses from snapshot: 16470 addresses\n",
      "Eligible Addresses: 15737\n",
      "\n",
      "Snapshot #1\n",
      "Eligible Addresses from snapshot: 16470 addresses\n",
      "Eligible Addresses: 15717\n",
      "\n",
      "Snapshot #2\n",
      "Eligible Addresses from snapshot: 16473 addresses\n",
      "Eligible Addresses: 15715\n",
      "\n",
      "Total Eligible Addresses (holders): 15715\n",
      "Initial Eligible Addresses (LP): 16470\n",
      "\n",
      "Snapshot #0\n",
      "Eligible Addresses from snapshot: 16470 addresses\n",
      "Eligible Addresses: 16470\n",
      "\n",
      "Snapshot #1\n",
      "Eligible Addresses from snapshot: 16473 addresses\n",
      "Eligible Addresses: 16467\n",
      "\n",
      "Total Eligible Addresses (LP): 16467\n",
      "Initial Eligible Addresses (stakers): 16470\n",
      "\n",
      "Snapshot #0\n",
      "Eligible Addresses from snapshot: 16470 addresses\n",
      "Eligible Addresses: 16470\n",
      "\n",
      "Snapshot #1\n",
      "Eligible Addresses from snapshot: 16473 addresses\n",
      "Eligible Addresses: 16467\n",
      "\n",
      "Total Eligible Addresses (stakers): 16467\n"
     ]
    }
   ],
   "source": [
    "def filter_addresses(initial_df, snapshot_df):\n",
    "    # Calculate intersection of eligible addresses between existing set and snapshot set\n",
    "    initial_set = set(initial_df['address'])\n",
    "    snapshot_set = set(snapshot_df['address'])\n",
    "    addresses_intersection = list(initial_set.intersection(snapshot_set))\n",
    "    \n",
    "    # Filter addresses based on whether they're contained on the intersection set or not\n",
    "    filtered_df = initial_df[initial_df.apply(lambda x: x['address'] in addresses_intersection, axis=1)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def get_eligible(initial_df, snapshots, category):\n",
    "    print()\n",
    "    print('Initial Eligible Addresses (%s): %s' % (category, len(initial_df.index)))\n",
    "    print()\n",
    "\n",
    "    eligible_df = initial_df\n",
    "\n",
    "    for index, snapshot in enumerate(snapshots):\n",
    "        print('Snapshot #%s' % index)\n",
    "        snapshot_eligible = snapshot[snapshot['balance'] >= AGI_THRESHOLD]\n",
    "        print('Eligible Addresses from snapshot: %s addresses' % len(snapshot_eligible.index))\n",
    "        eligible_df = filter_addresses(eligible_df, snapshot_eligible)\n",
    "        print('Eligible Addresses: %s' % len(eligible_df.index))\n",
    "        print()\n",
    "\n",
    "    print('Total Eligible Addresses (%s): %s' % (category, len(eligible_df.index)))\n",
    "    \n",
    "\n",
    "eligible_addresses_holders = get_eligible(eligible_addresses_holders, holders_snapshots, 'holders')\n",
    "eligible_addresses_lp = get_eligible(eligible_addresses_lp, lp_snapshots, 'LP')\n",
    "eligible_addresses_stakers = get_eligible(eligible_addresses_stakers, stakers_snapshots, 'stakers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating airdrop amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decimals SDAO\n",
    "\n",
    "DECIMALS_SDAO = 18\n",
    "\n",
    "CONVERT_TO_FULL_BALANCE_SDAO = np.power(np.ulonglong(10), DECIMALS_SDAO)\n",
    "\n",
    "# Reward parameters\n",
    "\n",
    "TOTAL_STAKING_REWARD = 550000.0\n",
    "\n",
    "TOTAL_REWARD = 825000.0\n",
    "\n",
    "# Adjust rewards to be full balance\n",
    "\n",
    "TOTAL_STAKING_REWARD *= CONVERT_TO_FULL_BALANCE_SDAO\n",
    "\n",
    "TOTAL_REWARD *= CONVERT_TO_FULL_BALANCE_SDAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stakers\n",
    "\n",
    "There are two kinds of rewards for stakers:\n",
    "- Per user (divided equally among staking wallets)\n",
    "- Per stake amount (delivered proportionally to the amounts staked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holders and LP\n",
    "\n",
    "Knowing the eligibility of the addresses, we can calculate the balances now using the following formula.\n",
    "\n",
    "With those premises in place, we can calculate the final reward for each user by using the following formula\n",
    "\n",
    "`Reward = total_reward * log10(1+user_balance) / SUM(log10(1+user_balance))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       holder_reward                                     address      balance\n",
      "0          72.108259  0xbe0eb53f46cd790cd13851d5eff43d12404d33e8  141823360.0\n",
      "1          71.509113  0xf977814e90da44bfa03b6295a0616a897441acec  104122164.0\n",
      "4          68.034971  0xa1d8d972560c2f8144af871db508f0b0b10a3fbf   17352399.0\n",
      "5          67.739481  0x19184ab45c40c2920b0e0e31413b9434abd243ed   14899506.0\n",
      "6          67.695774  0x8699b0ffff9136df5fed0175baf4b65477378a3d   14567393.0\n",
      "...              ...                                         ...          ...\n",
      "16684      49.108670  0xc4b4e449432171c7b945c4a8af36b107f8d70df4       1000.0\n",
      "16685      49.108670  0xa6e615df1824cc71eb7ae5a823c5808ea2b196fc       1000.0\n",
      "16686      49.108670  0x369fb3e7cebc8e4bc2d6166f9e5139d564c38db4       1000.0\n",
      "16687      49.108670  0x1da14e9cdae510ae75972f5221200fa781fdb4c8       1000.0\n",
      "16688      49.108670  0xba3699d5fc6276e87c3a761eaaf82162d4a86854       1000.0\n",
      "\n",
      "[15715 rows x 3 columns]\n",
      "Allocated reward (holders and LP): 825000.0\n",
      "Calculated reward (holders and LP): 825000.0\n"
     ]
    }
   ],
   "source": [
    "# Define SUM(log10(1+user_balance)) as a constant variable\n",
    "\n",
    "balances = list(eligible_addresses['balance'])\n",
    "\n",
    "balances_log10 = [np.log10(1 + (balance)) for balance in balances]\n",
    "\n",
    "sum_balances_log10 = np.sum(balances_log10)\n",
    "\n",
    "# Define the function that calculates the reward for each user\n",
    "\n",
    "def calculate_holder_reward(total_reward, user_balance_index):\n",
    "    user_balance_log10 = balances_log10[user_balance_index]\n",
    "    reward_percentage = np.double(user_balance_log10) / np.double(sum_balances_log10)\n",
    "    # Calculate reward and convert to final balance\n",
    "    # Move the position of the floating point to have more precision around small percentages\n",
    "    return (total_reward * (reward_percentage * np.power(10, 9)) / np.power(10, 9)) / CONVERT_TO_FULL_BALANCE_SDAO\n",
    "\n",
    "\n",
    "# Calculate rewards and add the SDAO value as a column to the DateFrame\n",
    "\n",
    "holder_rewards = [calculate_holder_reward(TOTAL_REWARD, index) for index, balance in enumerate(balances)]\n",
    "\n",
    "rewards_df = eligible_addresses.copy()\n",
    "\n",
    "rewards_df.insert(0, 'holder_reward', holder_rewards)\n",
    "\n",
    "rewards_df['balance'] = rewards_df['balance'].apply(lambda x: x / CONVERT_TO_FULL_BALANCE_AGI)\n",
    "\n",
    "print(rewards_df)\n",
    "\n",
    "# Verify that the total amount of allocated reward matches the expected value\n",
    "\n",
    "calculated_holder_reward = np.sum(list(rewards_df['holder_reward']))\n",
    "\n",
    "adjusted_total_holder_reward = (TOTAL_REWARD / CONVERT_TO_FULL_BALANCE_SDAO)\n",
    "\n",
    "print('Allocated reward (holders and LP): %s' % adjusted_total_holder_reward)\n",
    "print('Calculated reward (holders and LP): %s' % calculated_holder_reward)\n",
    "\n",
    "if np.ulonglong(calculated_holder_reward) != adjusted_total_holder_reward:\n",
    "    raise Exception('Error calculating rewards', 'final reward sum does not match allocated reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding unclaimed balances\n",
    "\n",
    "The missing step would be sum all the unclaimed amount from previous airdrops, for this example that's not possible with the data at hand though."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e11c16bd8665eb0b93b5a4e36de9de775b152bd48248748b339502fe1d55318"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
